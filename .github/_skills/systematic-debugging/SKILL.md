---
name: systematic-debugging
description: Use when debugging technical issues, especially under time pressure, after multiple failed fix attempts, or when you don't fully understand the root cause. Prevents random fixes and ensures systematic root cause investigation before attempting solutions.
---

# Systematic Debugging

## Overview

Random fixes waste time and create new bugs. Quick patches mask underlying issues.

**Core principle:** ALWAYS find root cause before attempting fixes. Symptom fixes are failure.

**Violating the letter of this process is violating the spirit of debugging.**

## When to Use

**Use this skill for ANY technical issue:**
- Test failures
- Bugs in production or development
- Unexpected behavior
- Performance problems
- Build failures
- Integration issues

**Use ESPECIALLY when:**
- Under time pressure (emergencies make guessing tempting)
- "Just one quick fix" seems obvious
- You've already tried multiple fixes
- Previous fix didn't work
- You don't fully understand the issue
- You're proposing solutions before understanding the problem

**Do NOT skip when:**
- Issue seems simple (simple bugs have root causes too)
- You're in a hurry (rushing guarantees rework)
- Manager wants it fixed NOW (systematic is faster than thrashing)

## The Iron Law

```
NO FIXES WITHOUT ROOT CAUSE INVESTIGATION FIRST
```

If you haven't completed Phase 1, you cannot propose fixes.

## The Four Phases

You MUST complete each phase before proceeding to the next.

### Phase 1: Root Cause Investigation

**BEFORE attempting ANY fix:**

#### 1. Read Error Messages Carefully
- Don't skip past errors or warnings
- They often contain the exact solution
- Read stack traces completely
- Note line numbers, file paths, error codes

#### 2. Reproduce Consistently
- Can you trigger it reliably?
- What are the exact steps?
- Does it happen every time?
- If not reproducible â†’ gather more data, don't guess

#### 3. Check Recent Changes
- What changed that could cause this?
- Git diff, recent commits
- New dependencies, config changes
- Environmental differences

#### 4. Gather Evidence in Multi-Component Systems

**WHEN system has multiple components (CI â†’ build â†’ signing, API â†’ service â†’ database):**

BEFORE proposing fixes, add diagnostic instrumentation:

```
For EACH component boundary:
  - Log what data enters component
  - Log what data exits component
  - Verify environment/config propagation
  - Check state at each layer

Run once to gather evidence showing WHERE it breaks
THEN analyze evidence to identify failing component
THEN investigate that specific component
```

**Example (multi-layer system):**
```bash
# Layer 1: Workflow
echo "=== Secrets available in workflow: ==="
echo "IDENTITY: ${IDENTITY:+SET}${IDENTITY:-UNSET}"

# Layer 2: Build script
echo "=== Env vars in build script: ==="
env | grep IDENTITY || echo "IDENTITY not in environment"

# Layer 3: Signing script
echo "=== Keychain state: ==="
security list-keychains
security find-identity -v

# Layer 4: Actual signing
codesign --sign "$IDENTITY" --verbose=4 "$APP"
```

**This reveals:** Which layer fails (secrets â†’ workflow âœ“, workflow â†’ build âœ—)

#### 5. Trace Data Flow

**WHEN error is deep in call stack:**

See `root-cause-tracing.md` in this directory for the complete backward tracing technique.

**Quick version:**
- Where does bad value originate?
- What called this with bad value?
- Keep tracing up until you find the source
- Fix at source, not at symptom

### Phase 2: Pattern Analysis

**Find the pattern before fixing:**

#### 1. Find Working Examples
- Locate similar working code in same codebase
- What works that's similar to what's broken?

#### 2. Compare Against References
- If implementing pattern, read reference implementation COMPLETELY
- Don't skim - read every line
- Understand the pattern fully before applying

#### 3. Identify Differences
- What's different between working and broken?
- List every difference, however small
- Don't assume "that can't matter"

#### 4. Understand Dependencies
- What other components does this need?
- What settings, config, environment?
- What assumptions does it make?

### Phase 3: Hypothesis and Testing

**Scientific method:**

#### 1. Form Single Hypothesis
- State clearly: "I think X is the root cause because Y"
- Write it down
- Be specific, not vague

#### 2. Test Minimally
- Make the SMALLEST possible change to test hypothesis
- One variable at a time
- Don't fix multiple things at once

#### 3. Verify Before Continuing
- Did it work? Yes â†’ Phase 4
- Didn't work? Form NEW hypothesis
- DON'T add more fixes on top

#### 4. When You Don't Know
- Say "I don't understand X"
- Don't pretend to know
- Ask for help
- Research more

### Phase 4: Implementation

**Fix the root cause, not the symptom:**

#### 1. Create Failing Test Case
- Simplest possible reproduction
- Automated test if possible
- One-off test script if no framework
- MUST have before fixing
- Use the test-driven-development workflow for writing proper failing tests

#### 2. Implement Single Fix
- Address the root cause identified
- ONE change at a time
- No "while I'm here" improvements
- No bundled refactoring

#### 3. Verify Fix
- Test passes now?
- No other tests broken?
- Issue actually resolved?

#### 4. If Fix Doesn't Work
- STOP
- Count: How many fixes have you tried?
- If < 3: Return to Phase 1, re-analyze with new information
- **If â‰¥ 3: STOP and question the architecture (step 5 below)**
- DON'T attempt Fix #4 without architectural discussion

#### 5. If 3+ Fixes Failed: Question Architecture

**Pattern indicating architectural problem:**
- Each fix reveals new shared state/coupling/problem in different place
- Fixes require "massive refactoring" to implement
- Each fix creates new symptoms elsewhere

**STOP and question fundamentals:**
- Is this pattern fundamentally sound?
- Are we "sticking with it through sheer inertia"?
- Should we refactor architecture vs. continue fixing symptoms?

**Discuss with your human partner before attempting more fixes**

This is NOT a failed hypothesis - this is a wrong architecture.

## Red Flags

Early warning signs that you're violating systematic debugging:

- ðŸš© "Quick fix for now, investigate later"
- ðŸš© "Just try changing X and see if it works"
- ðŸš© "Add multiple changes, run tests"
- ðŸš© "Skip the test, I'll manually verify"
- ðŸš© "It's probably X, let me fix that"
- ðŸš© "I don't fully understand but this might work"
- ðŸš© "Pattern says X but I'll adapt it differently"
- ðŸš© "Here are the main problems:" [lists fixes without investigation]
- ðŸš© Proposing solutions before tracing data flow
- ðŸš© "One more fix attempt" (when already tried 2+)
- ðŸš© Each fix reveals new problem in different place

**ALL of these mean: STOP. Return to Phase 1.**

**If 3+ fixes failed:** Question the architecture (see Phase 4.5)

## Human Partner's Signals You're Doing It Wrong

Watch for these redirections:
- "Is that not happening?" â†’ You assumed without verifying
- "Will it show us...?" â†’ You should have added evidence gathering
- "Stop guessing" â†’ You're proposing fixes without understanding
- "Ultrathink this" â†’ Question fundamentals, not just symptoms
- "We're stuck?" (frustrated) â†’ Your approach isn't working

**When you see these:** STOP. Return to Phase 1.

## Rationalization Counters

| Rationalization | Why It's Wrong | Counter-Argument |
|-----------------|----------------|------------------|
| "Issue is simple, don't need process" | Simple issues have root causes too | Process is fast for simple bugs. Skip it = risk rework. |
| "Emergency, no time for process" | Thrashing wastes more time | Systematic debugging is FASTER than guess-and-check. |
| "Just try this first, then investigate" | Sets bad pattern | First fix sets the precedent. Do it right from the start. |
| "I'll write test after confirming fix works" | Untested fixes don't stick | Test first proves the fix. No test = no verification. |
| "Multiple fixes at once saves time" | Can't isolate root cause | Creates new bugs. Can't tell what worked. |
| "Reference too long, I'll adapt the pattern" | Partial understanding | Incomplete knowledge guarantees bugs. Read completely. |
| "I see the problem, let me fix it" | Symptoms â‰  root cause | Seeing symptoms is not understanding. Investigate first. |
| "One more fix attempt" (after 2+) | Pattern problem, not hypothesis | 3+ failures = architectural issue. Question the design. |

## Quick Reference

| Phase | Key Activities | Success Criteria |
|-------|---------------|------------------|
| **1. Root Cause** | Read errors, reproduce, check changes, gather evidence | Understand WHAT and WHY |
| **2. Pattern** | Find working examples, compare differences | Identify what's different |
| **3. Hypothesis** | Form theory, test minimally | Confirmed or new hypothesis |
| **4. Implementation** | Create test, fix once, verify | Bug resolved, tests pass |

## Supporting Techniques

These techniques are available in this directory:

- **`root-cause-tracing.md`** - Trace bugs backward through call stack to find original trigger
- **`defense-in-depth.md`** - Add validation at multiple layers after finding root cause
- **`condition-based-waiting.md`** - Replace arbitrary timeouts with condition polling

## Related Workflows

- **Test-Driven Development** (`.agent/workflows/test-driven-development.md`) - For creating failing test case (Phase 4, Step 1)
- **Verification Before Completion** (`.agent/workflows/verification-before-completion.md`) - Verify fix worked before claiming success

## Real-World Impact

**Without systematic debugging:**
- 2-3 hours of thrashing
- First-time fix rate: 40%
- New bugs commonly introduced

**With systematic debugging:**
- 15-30 minutes to fix
- First-time fix rate: 95%
- New bugs: Near zero
